{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 01 - Pré-processamento de Dados\n",
    "## Classificação de Desinformação Digital - Liga Acadêmica de IA (Ligia/UFPE) 2026\n",
    "\n",
    "**Objetivo:** Preparar os dados brutos para a etapa de modelagem, executando as seguintes transformações em ordem determinística: remoção de data leakage, extração de features de estilo, lematização e vetorização TF-IDF.\n",
    "\n",
    "**Pré-requisito:** `notebook_00_EDA.ipynb` | **Próximo passo:** `notebook_02_modeling.ipynb`\n",
    "\n",
    "Todas as transformações aplicadas ao conjunto de treino são aprendidas **exclusivamente a partir do treino** e apenas aplicadas (sem re-ajuste) ao conjunto de teste, garantindo ausência de data leakage em qualquer etapa da pipeline.\n",
    "\n",
    "| Componente | Baseline | Otimizado | Justificativa |\n",
    "|---|---|---|---|\n",
    "| TF-IDF `max_features` | 5.000 | 12.000 | Vocabulário mais rico captura termos compostos |\n",
    "| `ngram_range` | (1,2) | (1,3) | Trigramas capturam padrões diagnósticos |\n",
    "| `min_df` / `max_df` | - | 2 / 0.95 | Remove hapax e stopwords remanescentes |\n",
    "| Features de estilo | 3 | 15 | EDA revelou features adicionais discriminativas |\n",
    "| Lematização | - | Sim (NLTK+POS) | Reduz variação morfológica preservando semântica |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importações e Configurações\n",
    "\n",
    "Além das bibliotecas padrão de análise e transformação, este notebook utiliza `scipy.sparse` para manipulação eficiente de matrizes esparsas - necessário dado que a matriz TF-IDF possui dimensionalidade de 12.000 colunas com alta esparsidade. A função `find_root()` detecta automaticamente a raiz do projeto, garantindo portabilidade entre ambientes (local, Kaggle, Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "code_imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR : C:\\Users\\gui05\\Pictures\\LIGIA_FINAL\n",
      "DATA_DIR : C:\\Users\\gui05\\Pictures\\LIGIA_FINAL\\outputs\\artifacts\n",
      "Modulo src/preprocessing.py importado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# ── Dependências do notebook ──────────────────────────────────────────────\n",
    "# scipy.sparse: manipulação de matrizes esparsas (TF-IDF + style features)\n",
    "# hstack: concatenação horizontal de matrizes esparsas sem densificação\n",
    "# save_npz: serialização eficiente de matrizes esparsas em disco\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack, save_npz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Detecta automaticamente a raiz do projeto ---\n",
    "# Sobe diretorios ate encontrar a pasta 'inputs/' com os CSVs\n",
    "def find_root() -> Path:\n",
    "    candidate = Path('.').resolve()\n",
    "    for _ in range(6):  # sobe ate 6 niveis\n",
    "        if (candidate / 'inputs' / 'train.csv').exists():\n",
    "            return candidate\n",
    "        candidate = candidate.parent\n",
    "    raise FileNotFoundError(\n",
    "        'Nao foi possivel encontrar a pasta inputs/train.csv. '\n",
    "        'Execute o notebook a partir da raiz do projeto LIGIA_FINAL.'\n",
    "    )\n",
    "\n",
    "ROOT_DIR = find_root()\n",
    "DATA_DIR = ROOT_DIR / 'outputs' / 'artifacts'\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'ROOT_DIR : {ROOT_DIR}')\n",
    "print(f'DATA_DIR : {DATA_DIR}')\n",
    "# ── Modulo compartilhado de pre-processamento ──────────────────────────────\n",
    "# As funcoes clean_text(), add_style_features(), lemmatize_series() e as\n",
    "# constantes STYLE_COLS, SENSATIONAL_TERMS, REUTERS_SUBJECTS estao centralizadas\n",
    "# em src/preprocessing.py para evitar duplicacao com notebook_03_inference.ipynb\n",
    "import sys\n",
    "# ROOT_DIR ja foi calculado acima pelo find_root() — garante que src/ seja encontrado\n",
    "# independentemente de onde o notebook e executado (notebooks/, raiz, etc.)\n",
    "if str(ROOT_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT_DIR))\n",
    "try:\n",
    "    from src.preprocessing import (\n",
    "        clean_text, add_style_features, lemmatize_series,\n",
    "        STYLE_COLS, SENSATIONAL_TERMS, REUTERS_SUBJECTS, _CLEAN_PATTERNS,\n",
    "    )\n",
    "    print(\"Modulo src/preprocessing.py importado com sucesso.\")\n",
    "except ImportError as e:\n",
    "    print(f\"[AVISO] src/preprocessing.py nao encontrado ({e}). Funcoes definidas localmente abaixo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Remoção de Data Leakage\n",
    "\n",
    "A EDA identificou que determinadas marcações textuais funcionam como identificadores diretos da classe, tornando a predição trivial e metodologicamente inválida. Especificamente, a assinatura `(Reuters)` está presente em mais de 99% dos artigos das categorias `politicsNews` e `worldnews`, enquanto fontes como `Infowars` e `Breitbart` são exclusivas da classe *fake*. Um modelo treinado sem remover esses artefatos aprenderia a identificar o veículo publicador, não os padrões linguísticos da desinformação.\n",
    "\n",
    "**Artefatos removidos:**\n",
    "- **Tags de localização/agência:** `WASHINGTON (Reuters) -` (exclusivas de notícias reais)\n",
    "- **Menções a agências:** `(Reuters)`, `(AP)`, `(AFP)`\n",
    "- **Fontes fake conhecidas:** `Infowars`, `Breitbart`, etc.\n",
    "- **URLs, mentions e artefatos de coleta**\n",
    "\n",
    "> **Importante:** As features de estilo são calculadas **antes** da limpeza de leakage, preservando o estilo tipográfico original do autor (maiúsculas, pontuação, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "code_regex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacao dos patterns:\n",
      "  Antes : (Reuters) - The president announced today https://reuters.com/article ...\n",
      "  Depois: - The president announced today ...\n",
      "\n",
      "  Antes : WASHINGTON (Reuters) - Senate passed the bill.\n",
      "  Depois: Senate passed the bill.\n",
      "\n",
      "  Antes : Infowars reports: shocking news!\n",
      "  Depois: reports: shocking news!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ── Padrões Regex para remoção de data leakage ────────────────────────────\n",
    "# RE_LOCATION_AGENCY: remove datelines no formato \"WASHINGTON (Reuters) —\"\n",
    "#   presente exclusivamente em notícias reais (agências jornalísticas)\n",
    "# RE_AGENCY_TAG: remove menções inline \"(Reuters)\", \"(AP)\", \"(AFP)\"\n",
    "# RE_FAKE_SOURCES: remove nomes de veículos exclusivamente associados a fake news\n",
    "# RE_URL / RE_MENTION: remove artefatos de coleta sem valor linguístico\n",
    "# RE_LEAKAGE_ARTIFACTS: padrões residuais (ex: \"21WIRE\", \"READ MORE\")\n",
    "RE_LOCATION_AGENCY   = re.compile(\n",
    "    r'^[A-Z][A-Z\\s/,\\.]+\\s*\\([A-Za-z\\s]+\\)\\s*[-\\u2013\\u2014]?\\s*', re.MULTILINE)\n",
    "RE_AGENCY_TAG        = re.compile(r'\\(\\s*(?:Reuters|AP|AFP)\\s*\\)', re.IGNORECASE)\n",
    "RE_FAKE_SOURCES      = re.compile(\n",
    "    r'\\b(?:21st\\s*Century\\s*Wire|YourNewsWire|Infowars|Breitbart|RT\\.com|NaturalNews|BeforeItsNews)\\b',\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "RE_URL               = re.compile(r'https?://\\S+|www\\.\\S+', re.IGNORECASE)\n",
    "RE_MENTION           = re.compile(r'@\\w+')\n",
    "RE_LEAKAGE_ARTIFACTS = re.compile(\n",
    "    r'\\b(video|image|featured|getty(\\s+images)?|bit\\.ly|pic\\.twitter\\.com|reuters)\\b',\n",
    "    re.IGNORECASE | re.MULTILINE,\n",
    ")\n",
    "\n",
    "_CLEAN_PATTERNS = (\n",
    "    RE_LOCATION_AGENCY, RE_AGENCY_TAG, RE_LEAKAGE_ARTIFACTS,\n",
    "    RE_FAKE_SOURCES, RE_URL, RE_MENTION,\n",
    ")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    for pattern in _CLEAN_PATTERNS:\n",
    "        text = pattern.sub('', str(text))\n",
    "    return re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "\n",
    "test_cases = [\n",
    "    '(Reuters) - The president announced today https://reuters.com/article ...',\n",
    "    'WASHINGTON (Reuters) - Senate passed the bill.',\n",
    "    'Infowars reports: shocking news!',\n",
    "]\n",
    "print('Validacao dos patterns:')\n",
    "for tc in test_cases:\n",
    "    print(f'  Antes : {tc}')\n",
    "    print(f'  Depois: {clean_text(tc)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Features de Estilo (15 features)\n",
    "\n",
    "Fundamentadas nos achados da EDA, estas features capturam dimensões linguísticas não representadas diretamente pelo conteúdo textual. A hipótese é que a desinformação apresenta assinaturas estilísticas mensuráveis - uso excessivo de maiúsculas, pontuação agressiva e vocabulário sensacionalista - que são **ortogonais** ao conteúdo semântico capturado pelo TF-IDF.\n",
    "\n",
    "As 15 features são divididas em três grupos funcionais:\n",
    "- **Volume textual:** `text_len`, `word_count`, `title_len`, `avg_word_len`\n",
    "- **Pontuação e tipografia:** `caps_ratio`, `title_caps_ratio`, `exclamation_count`, `question_count`, `ellipsis_count`, `quote_count`, `all_caps_words`\n",
    "- **Léxico e estilo:** `sentence_count`, `avg_sentence_len`, `unique_word_ratio`, `sensational_count`\n",
    "\n",
    "> **Ordem crítica:** Estas features são extraídas do **texto bruto**, antes da remoção de leakage, para preservar o estilo tipográfico original do autor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "code_style",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features de estilo definidas: 15 colunas\n",
      "['caps_ratio', 'exclamation_count', 'title_len', 'text_len', 'word_count', 'avg_word_len', 'sentence_count', 'avg_sentence_len', 'question_count', 'quote_count', 'ellipsis_count', 'all_caps_words', 'title_caps_ratio', 'unique_word_ratio', 'sensational_count']\n"
     ]
    }
   ],
   "source": [
    "# ── Extração das 15 features de estilo ────────────────────────────────────\n",
    "# Calculadas no texto BRUTO (antes da limpeza) — ver justificativa na Seção 3\n",
    "# caps_ratio: proporção de caracteres alfabéticos em maiúsculas no corpo do texto\n",
    "# unique_word_ratio: diversidade lexical = palavras únicas / total de palavras\n",
    "# sensational_count: contagem de termos de alarme/urgência (lista curada)\n",
    "SENSATIONAL_RE = re.compile(\n",
    "    r'\\b(shocking|unbelievable|amazing|incredible|must see|breaking|exclusive|urgent)\\b',\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "STYLE_COLS = [\n",
    "    'caps_ratio', 'exclamation_count', 'title_len',\n",
    "    'text_len', 'word_count', 'avg_word_len',\n",
    "    'sentence_count', 'avg_sentence_len',\n",
    "    'question_count', 'quote_count', 'ellipsis_count',\n",
    "    'all_caps_words', 'title_caps_ratio',\n",
    "    'unique_word_ratio', 'sensational_count',\n",
    "]\n",
    "\n",
    "def _caps_ratio(text: str) -> float:\n",
    "    alpha = [c for c in str(text) if c.isalpha()]\n",
    "    return sum(c.isupper() for c in alpha) / len(alpha) if alpha else 0.0\n",
    "\n",
    "def _avg_word_len(text: str) -> float:\n",
    "    words = str(text).split()\n",
    "    return float(np.mean([len(w) for w in words])) if words else 0.0\n",
    "\n",
    "def _unique_word_ratio(text: str) -> float:\n",
    "    words = str(text).split()\n",
    "    return len(set(words)) / len(words) if words else 0.0\n",
    "\n",
    "def _quote_count(text: str) -> int:\n",
    "    return sum(1 for c in str(text) if c in ('\"', \"'\", '\\u2018', '\\u2019', '\\u201c', '\\u201d'))\n",
    "\n",
    "def add_style_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['caps_ratio']        = df['text'].apply(_caps_ratio)\n",
    "    df['exclamation_count'] = df['text'].str.count('!')\n",
    "    df['title_len']         = df['title'].str.len()\n",
    "    df['text_len']          = df['text'].str.len()\n",
    "    df['word_count']        = df['text'].str.split().str.len()\n",
    "    df['avg_word_len']      = df['text'].apply(_avg_word_len)\n",
    "    df['sentence_count']    = df['text'].str.count(r'[.!?]+')\n",
    "    df['avg_sentence_len']  = df['word_count'] / df['sentence_count'].replace(0, 1)\n",
    "    df['question_count']    = df['text'].str.count(r'\\?')\n",
    "    df['quote_count']       = df['text'].apply(_quote_count)\n",
    "    df['ellipsis_count']    = df['text'].str.count(r'\\.{2,}')\n",
    "    df['all_caps_words']    = df['text'].str.findall(r'\\b[A-Z]{2,}\\b').str.len()\n",
    "    df['title_caps_ratio']  = df['title'].apply(_caps_ratio)\n",
    "    df['unique_word_ratio'] = df['text'].apply(_unique_word_ratio)\n",
    "    df['sensational_count'] = df['text'].apply(lambda x: len(SENSATIONAL_RE.findall(str(x))))\n",
    "    return df\n",
    "\n",
    "print(f'Features de estilo definidas: {len(STYLE_COLS)} colunas')\n",
    "print(STYLE_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline de Pré-processamento\n",
    "\n",
    "A função `preprocess()` encapsula as quatro etapas de transformação em uma ordem determinística, projetada para eliminar qualquer possibilidade de contaminação entre treino e teste:\n",
    "\n",
    "1. **Extração de features de estilo** no texto bruto - antes de qualquer limpeza\n",
    "2. **Remoção de leakage** via expressões regulares\n",
    "3. **Concatenação** de título limpo + texto limpo em `clean_text`\n",
    "4. **Remoção de colunas originais** para evitar uso acidental nas etapas seguintes\n",
    "\n",
    "A função recebe o parâmetro `split` (ex: `\"TRAIN\"`, `\"TEST\"`) exclusivamente para fins de logging - a lógica de transformação é idêntica para ambos os conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "code_pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline definido.\n"
     ]
    }
   ],
   "source": [
    "_DROP = ['subject', 'date', 'title', 'text', 'title_clean', 'text_clean']\n",
    "\n",
    "def preprocess(df: pd.DataFrame, split: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Etapas em ordem determinística — a sequência é crítica para evitar leakage:\n",
    "    1. Features de estilo NO TEXTO BRUTO (antes de qualquer limpeza)\n",
    "    2. Limpeza de leakage\n",
    "    3. Concatenacao titulo+texto limpos\n",
    "    4. Remoção das colunas originais (title, text, date) — evita uso acidental na modelagem\n",
    "    \"\"\"\n",
    "    print(f'[{split}] Shape inicial: {df.shape}')\n",
    "    df = df.copy()\n",
    "    df['title'] = df['title'].fillna('')\n",
    "    df['text']  = df['text'].fillna('')\n",
    "    df = add_style_features(df)\n",
    "    df['title_clean'] = df['title'].apply(clean_text)\n",
    "    df['text_clean']  = df['text'].apply(clean_text)\n",
    "    df['clean_text']  = (df['title_clean'] + ' ' + df['text_clean']).str.strip()\n",
    "    df = df.drop(columns=[c for c in _DROP if c in df.columns])\n",
    "    print(f'[{split}] Shape final  : {df.shape}')\n",
    "    return df\n",
    "\n",
    "print('Pipeline definido.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Carregamento e Validação dos Dados\n",
    "\n",
    "Antes de aplicar qualquer transformação, verifica-se a integridade estrutural dos dados: presença das colunas esperadas e ausência de sobreposição de IDs entre treino e teste. A validação de overlap é uma salvaguarda contra data leakage no nível de instâncias - se um exemplo de teste aparecesse no treino, o modelo teria vantagem injusta na avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "code_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : (22844, 6)  |  Test : (5712, 5)\n",
      "Colunas train: ['id', 'title', 'text', 'subject', 'date', 'label']\n",
      "Validacao de overlap: OK (0 IDs comuns)\n",
      "Validacao NaN em label: OK\n",
      "\n",
      "Distribuicao de classes (train):\n",
      "label\n",
      "Real (0)    17133\n",
      "Fake (1)     5711\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ── Carregamento e validação de integridade dos datasets ──────────────────\n",
    "# O assert de overlap verifica que nenhum ID do teste está no treino\n",
    "# (data leakage em nível de instância — invalidaria a avaliação competitiva)\n",
    "train_path = ROOT_DIR / 'inputs' / 'train.csv'\n",
    "test_path  = ROOT_DIR / 'inputs' / 'test.csv'\n",
    "\n",
    "df_train_raw = pd.read_csv(train_path)\n",
    "df_test_raw  = pd.read_csv(test_path)\n",
    "\n",
    "print(f'Train : {df_train_raw.shape}  |  Test : {df_test_raw.shape}')\n",
    "print(f'Colunas train: {list(df_train_raw.columns)}')\n",
    "\n",
    "# Validacao: sem overlap de IDs\n",
    "assert 'id' in df_train_raw.columns and 'id' in df_test_raw.columns\n",
    "overlap = set(df_train_raw['id']) & set(df_test_raw['id'])\n",
    "assert len(overlap) == 0, f'Data leakage: {len(overlap)} IDs comuns!'\n",
    "print('Validacao de overlap: OK (0 IDs comuns)')\n",
    "\n",
    "# Validacao: sem NaN na label\n",
    "assert df_train_raw['label'].isna().sum() == 0\n",
    "print('Validacao NaN em label: OK')\n",
    "\n",
    "print(f'\\nDistribuicao de classes (train):')\n",
    "print(df_train_raw['label'].value_counts().rename({0: 'Real (0)', 1: 'Fake (1)'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aplicação do Pipeline\n",
    "\n",
    "O pipeline é aplicado independentemente a treino e teste. O assert final verifica que todas as 15 features de estilo foram criadas nos dois conjuntos, garantindo que a matriz de features terá o mesmo número de colunas durante treino e inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "code_apply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Shape inicial: (22844, 6)\n",
      "[TRAIN] Shape final  : (22844, 18)\n",
      "[TEST] Shape inicial: (5712, 5)\n",
      "[TEST] Shape final  : (5712, 17)\n",
      "\n",
      "Integridade das features: OK\n"
     ]
    }
   ],
   "source": [
    "# ── Aplicação do pipeline de pré-processamento ────────────────────────────\n",
    "# O pipeline é aplicado de forma idêntica a TRAIN e TEST\n",
    "# O assert verifica consistência de colunas antes da vetorização\n",
    "df_train = preprocess(df_train_raw, 'TRAIN')\n",
    "df_test  = preprocess(df_test_raw,  'TEST')\n",
    "\n",
    "assert all(c in df_test.columns for c in STYLE_COLS), 'Features de estilo faltando no test!'\n",
    "print('\\nIntegridade das features: OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lematização com NLTK\n",
    "\n",
    "A lematização com POS tagging reduz formas flexionadas à sua forma canônica: `running` → `run`, `better` → `good`, `policies` → `policy`. Diferentemente do stemming (que trunca sufixos mecanicamente), a lematização com `WordNetLemmatizer` utiliza o contexto morfossintático da palavra (verbo, substantivo, adjetivo) para produzir formas válidas no dicionário.\n",
    "\n",
    "**Justificativa:** Reduz a dimensionalidade efetiva do vocabulário sem perda semântica, consolidando variantes morfológicas de um mesmo conceito em um único token. Isso diminui o overfitting do TF-IDF a formas específicas de palavras.\n",
    "\n",
    "**Custo computacional:** A combinação de tokenização + POS tagging + lematização tem custo O(n) no número de tokens. Para o volume do dataset, o processo pode levar alguns minutos - o progresso é reportado via `tqdm`.\n",
    "\n",
    "> **Recursos NLTK necessários:** `punkt_tab`, `wordnet`, `stopwords`, `averaged_perceptron_tagger_eng` - baixados automaticamente na célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "code_lemma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematizando TRAIN...\n",
      "  [TRAIN] Lematizacao: 22,844 textos em 146.2s\n",
      "Lematizando TEST...\n",
      "  [TEST] Lematizacao: 5,712 textos em 35.8s\n"
     ]
    }
   ],
   "source": [
    "# ── Lematização com POS tagging (NLTK) ────────────────────────────────────\n",
    "# POS tagging (Part-of-Speech) melhora a qualidade da lematização ao fornecer\n",
    "# o contexto gramatical de cada token ao WordNetLemmatizer\n",
    "# _POS_MAP: converte tags Penn Treebank (JJ, VB, NN, RB) para tags WordNet\n",
    "# Tokens com len < 2 e stopwords são removidos para reduzir ruído\n",
    "for resource in ['punkt_tab', 'wordnet', 'stopwords', 'averaged_perceptron_tagger_eng']:\n",
    "    nltk.download(resource, quiet=True)\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "_lemmatizer = WordNetLemmatizer()\n",
    "_stop_words = set(stopwords.words('english'))\n",
    "_POS_MAP    = {'J': wordnet.ADJ, 'V': wordnet.VERB, 'N': wordnet.NOUN, 'R': wordnet.ADV}\n",
    "\n",
    "def _wn_pos(tag: str) -> str:\n",
    "    return _POS_MAP.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "def lemmatize_series(texts: pd.Series, split: str = '') -> pd.Series:\n",
    "    t0     = time.time()\n",
    "    result = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "        lemmas = [\n",
    "            _lemmatizer.lemmatize(w, _wn_pos(t))\n",
    "            for w, t in pos_tag(tokens)\n",
    "            if w.isalpha() and len(w) > 1 and w not in _stop_words\n",
    "        ]\n",
    "        result.append(' '.join(lemmas))\n",
    "    elapsed = time.time() - t0\n",
    "    print(f'  [{split}] Lematizacao: {len(texts):,} textos em {elapsed:.1f}s')\n",
    "    return pd.Series(result, index=texts.index)\n",
    "\n",
    "print('Lematizando TRAIN...')\n",
    "df_train['clean_text'] = lemmatize_series(df_train['clean_text'], 'TRAIN')\n",
    "print('Lematizando TEST...')\n",
    "df_test['clean_text']  = lemmatize_series(df_test['clean_text'], 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vetorização TF-IDF Otimizada\n",
    "\n",
    "O TF-IDF (Term Frequency–Inverse Document Frequency) representa cada documento como um vetor esparso no espaço de termos, ponderando a relevância de cada termo pela sua frequência no documento e pela sua raridade no corpus. Os parâmetros abaixo foram definidos com base nos achados da EDA e em experimentos de ablação realizados durante o desenvolvimento.\n",
    "\n",
    "| Parâmetro | Valor | Motivação |\n",
    "|---|---|---|\n",
    "| `max_features` | 12.000 | Vocabulário mais rico que o baseline (5.000) |\n",
    "| `ngram_range` | (1,3) | Trigramas capturam padrões compostos diagnósticos |\n",
    "| `min_df` | 2 | Remove hapax legomena (ruído estatístico) |\n",
    "| `max_df` | 0.95 | Remove stopwords remanescentes |\n",
    "| `sublinear_tf` | True | log(1+TF) reduz o efeito de termos muito frequentes |\n",
    "\n",
    "> O TF-IDF é **fitado exclusivamente no TRAIN** e apenas transformado no TEST, prevenindo data leakage de vocabulário "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "code_tfidf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF - Train: (22844, 12000)  |  Test: (5712, 12000)\n",
      "Vocabulario efetivo: 12,000 termos\n"
     ]
    }
   ],
   "source": [
    "# ── Vetorização TF-IDF ────────────────────────────────────────────────────\n",
    "# fit_transform no TRAIN: aprende o vocabulário e transforma em uma operação\n",
    "# transform no TEST: aplica o vocabulário aprendido sem modificá-lo (anti-leakage)\n",
    "# Resultado: matrizes esparsas de shape (n_docs, 12_000)\n",
    "X_train_text = df_train['clean_text']\n",
    "y_train      = df_train['label']\n",
    "X_test_text  = df_test['clean_text']\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_features=12_000,\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(1, 3),\n",
    ")\n",
    "\n",
    "# FIT apenas no TRAIN - transform separado para evitar leakage\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf  = tfidf.transform(X_test_text)\n",
    "\n",
    "print(f'TF-IDF - Train: {X_train_tfidf.shape}  |  Test: {X_test_tfidf.shape}')\n",
    "print(f'Vocabulario efetivo: {len(tfidf.vocabulary_):,} termos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Combinação Final: TF-IDF + Style Features (scaled)\n",
    "\n",
    "A matriz final de features combina dois tipos de representação complementares:\n",
    "\n",
    "| Componente | Tipo | Notas |\n",
    "|---|---|---|\n",
    "| TF-IDF | Esparso | 12.000 termos, 1–3grams, sublinear_tf |\n",
    "| Style features (scaled) | Denso convertido a esparso | 15 features, MaxAbsScaler |\n",
    "\n",
    "**Por que `MaxAbsScaler` e não `StandardScaler`?**\n",
    "O `StandardScaler` subtrai a média de cada feature, o que **densificaria** os zeros da matriz TF-IDF esparsa, resultando em uma matriz densa de ~12.000 colunas e potencial estouro de memória RAM. O `MaxAbsScaler` escala os valores para o intervalo [-1, 1] sem deslocar o centro, preservando a estrutura esparsa e permitindo a concatenação horizontal eficiente via `hstack`.\n",
    "\n",
    "> **Decisão Técnica (Anti-Leakage):** Tanto o TF-IDF quanto o `MaxAbsScaler` são ajustados (`fit`) **exclusivamente no conjunto de TREINO** e aplicados (`transform`) no TEST, prevenindo leakage de escala e vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "code_combine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composicao da matriz final:\n",
      "  TF-IDF features : 12,000\n",
      "  Style features  : 15 (scaled)\n",
      "  Total features  : 12,015\n",
      "  X_train         : (22844, 12015)\n",
      "  X_test          : (5712, 12015)\n",
      "Validacao NaN/Inf: OK\n"
     ]
    }
   ],
   "source": [
    "# ── Escalonamento das features de estilo e concatenação final ───────────\n",
    "# MaxAbsScaler: fit no TRAIN, transform no TEST (anti-leakage de escala)\n",
    "# csr_matrix: converte o array denso de style features para formato esparso\n",
    "#   → permite hstack com a matriz TF-IDF sem densificar nenhuma das duas\n",
    "# hstack: concatenação horizontal eficiente em memória (scipy.sparse)\n",
    "# --- Scaling das features de estilo (MaxAbsScaler) ---\n",
    "# Fit apenas no TRAIN, transform no TEST — evita leakage de escala\n",
    "style_scaler = MaxAbsScaler()\n",
    "X_train_style_arr = df_train[STYLE_COLS].values.astype(float)\n",
    "X_test_style_arr  = df_test[STYLE_COLS].values.astype(float)\n",
    "\n",
    "X_train_style = csr_matrix(style_scaler.fit_transform(X_train_style_arr))\n",
    "X_test_style  = csr_matrix(style_scaler.transform(X_test_style_arr))\n",
    "\n",
    "# --- Combinacao final: TF-IDF + Style (scaled) ---\n",
    "X_train_final = hstack([X_train_tfidf, X_train_style]).tocsr()\n",
    "X_test_final  = hstack([X_test_tfidf,  X_test_style]).tocsr()\n",
    "\n",
    "print('Composicao da matriz final:')\n",
    "print(f'  TF-IDF features : {X_train_tfidf.shape[1]:,}')\n",
    "print(f'  Style features  : {len(STYLE_COLS)} (scaled)')\n",
    "print(f'  Total features  : {X_train_final.shape[1]:,}')\n",
    "print(f'  X_train         : {X_train_final.shape}')\n",
    "print(f'  X_test          : {X_test_final.shape}')\n",
    "\n",
    "# Validacao anti-NaN\n",
    "assert not np.isnan(X_train_style_arr).any(), 'NaN nas features de estilo (train)!'\n",
    "assert not np.isnan(X_test_style_arr).any(),  'NaN nas features de estilo (test)!'\n",
    "print('Validacao NaN/Inf: OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Salvamento dos Artefatos\n",
    "\n",
    "Todos os artefatos gerados nesta etapa são serializados em disco para uso nas etapas seguintes (modelagem e inferência). É fundamental salvar não apenas as matrizes de features, mas também os transformadores ajustados (`tfidf_vectorizer.pkl`, `style_scaler.pkl`), pois eles serão necessários para processar novos documentos em inferência com o **mesmo vocabulário e escala** utilizados no treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "code_save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artefatos salvos em: C:\\Users\\gui05\\Pictures\\LIGIA_FINAL\\outputs\\artifacts\n",
      "  X_train.npz            (22844, 12015)\n",
      "  X_test.npz             (5712, 12015)\n",
      "  y_train.csv\n",
      "  test_ids.csv\n",
      "  tfidf_vectorizer.pkl   12,000 termos\n",
      "  style_scaler.pkl\n",
      "  train_preprocessed.csv\n",
      "  test_preprocessed.csv\n",
      "\n",
      "Proximo passo: notebook_02_modeling.ipynb\n"
     ]
    }
   ],
   "source": [
    "# ── Serialização dos artefatos de pré-processamento ──────────────────────\n",
    "# X_train.npz / X_test.npz: matrizes esparsas finais (TF-IDF + style, scaled)\n",
    "# y_train.csv: rótulos de treino alinhados por índice com X_train\n",
    "# tfidf_vectorizer.pkl: vocabulário e pesos IDF aprendidos no treino\n",
    "# style_scaler.pkl: parâmetros de escala (MaxAbs) aprendidos no treino\n",
    "# train/test_preprocessed.csv: DataFrames com features de estilo para análise\n",
    "save_npz(DATA_DIR / 'X_train.npz', X_train_final)\n",
    "save_npz(DATA_DIR / 'X_test.npz',  X_test_final)\n",
    "\n",
    "y_train.to_csv(DATA_DIR / 'y_train.csv', index=False)\n",
    "df_test[['id']].to_csv(DATA_DIR / 'test_ids.csv', index=False)\n",
    "\n",
    "with open(DATA_DIR / 'tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "with open(DATA_DIR / 'style_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(style_scaler, f)\n",
    "\n",
    "df_train.to_csv(DATA_DIR / 'train_preprocessed.csv', index=False)\n",
    "df_test.to_csv(DATA_DIR / 'test_preprocessed.csv', index=False)\n",
    "\n",
    "print(f'Artefatos salvos em: {DATA_DIR}')\n",
    "print(f'  X_train.npz            {X_train_final.shape}')\n",
    "print(f'  X_test.npz             {X_test_final.shape}')\n",
    "print(f'  y_train.csv')\n",
    "print(f'  test_ids.csv')\n",
    "print(f'  tfidf_vectorizer.pkl   {len(tfidf.vocabulary_):,} termos')\n",
    "print(f'  style_scaler.pkl')\n",
    "print(f'  train_preprocessed.csv')\n",
    "print(f'  test_preprocessed.csv')\n",
    "print(f'\\nProximo passo: notebook_02_modeling.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
